{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#2656a3;\">**Data Engineering and Machine Learning Operations in Business** </span> <span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Training Pipeline</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Feature selection.\n",
    "2. Feature transformations.\n",
    "3. Training datasets creation.\n",
    "4. Loading the training data.\n",
    "5. Train the model.\n",
    "6. Register model to Hopsworks model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'> ‚öôÔ∏è Import of libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the packages for the needed libraries for the Jupyter notebook\n",
    "import inspect \n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üì° Connecting to Hopsworks Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/556180\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Importing the hopsworks module\n",
    "import hopsworks\n",
    "\n",
    "# Logging in to the Hopsworks project\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Getting the feature store from the project\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the feature groups\n",
    "electricity_fg = fs.get_feature_group(\n",
    "    name='electricity_prices',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather_measurements',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "danish_holidays_fg = fs.get_feature_group(\n",
    "    name='danish_holidayss',\n",
    "    version=1,\n",
    ")\n",
    "forecast_renewable_energy_fg = fs.get_feature_group(\n",
    "    name='forecast_renewable_energy',\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üñç Feature View Creation and Retrieving </span>\n",
    "\n",
    "We first select the features that we want to include for model training.\n",
    "\n",
    "Since we specified `primary_key`as `date` and `event_time` as `timestamp` in part 01 we can now join them together for the `electricity_fg`, `weather_fg` and `forecast_renewable_energy_fg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm skal 'time' egentlig v√¶re 'date'???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "selected_features = electricity_fg.select_all()\\\n",
    "    .join(weather_fg.select_except([\"timestamp\", \"time\"]))\\\n",
    "    .join(forecast_renewable_energy_fg.select_except([\"timestamp\", \"time\"]))\\\n",
    "    .join(danish_holidays_fg.select_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you would like to view your selected features\n",
    "# selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\"> ü§ñ Transformation Functions</span>\n",
    "\n",
    "We preprocess our data using *min-max scaling* on the numerical features and *label encoding* on the one categorical feature we have.\n",
    "To achieve this, we create a mapping between our features and transformation functions. This ensures that transformation functions like min-max scaling are applied exclusively on the training data, preventing any data leakage into the validation or test sets.\n",
    "\n",
    "To achieve this, we create a mapping between our features and transformation functions - ved ikke om man kan sige det her?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transformation functions for feature scaling and encoding\n",
    "transformation_functions = {\n",
    "        \"dk1_spotpricedkk_kwh\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"dk1_offshore_wind_forecastintraday_kwh\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"dk1_onshore_wind_forecastintraday_kwh\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"dk1_solar_forecastintraday_kwh\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"temperature_2m\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"relative_humidity_2m\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"precipitation\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"rain\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"snowfall\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"weather_code\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"cloud_cover\": fs.get_transformation_function(name=\"min_max_scaler\"), \n",
    "        \"wind_speed_10m\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "        \"wind_gusts_10m\": fs.get_transformation_function(name=\"min_max_scaler\"),\n",
    "        \"type\": fs.get_transformation_function(name=\"label_encoder\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Feature Views` stands between **Feature Groups** and **Training Dataset**. –°ombining **Feature Groups** we can create **Feature Views** which store a metadata of our data. Having **Feature Views** we can create **Training Dataset**.\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "\n",
    "In order to create Feature View we can use `FeatureStore.get_or_create_feature_view()` method.\n",
    "\n",
    "We can specify next parameters:\n",
    "\n",
    "- `name` - name of a feature group.\n",
    "\n",
    "- `version` - version of a feature group.\n",
    "\n",
    "- `labels`- our target variable.\n",
    "\n",
    "- `transformation_functions` - functions to transform our features.\n",
    "\n",
    "- `query` - query object with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ved ikke om den her omformulering af botten g√•r an?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Feature Views` serve as an intermediary between **Feature Groups** and the **Training Dataset**. By combining various **Feature Groups**, we can construct **Feature Views**, which retain metadata about our data. Utilizing **Feature Views**, we can subsequently generate a **Training Dataset**.\n",
    "\n",
    "Feature Views facilitate the definition of schema through queries with filters, identification of the model's target feature or label, and application of additional transformation functions.\n",
    "\n",
    "To create a Feature View, we employ the `FeatureStore.get_or_create_feature_view()` method, where we specify the following parameters:\n",
    "\n",
    "- `name`: The name of the feature group.\n",
    "\n",
    "- `version`: The version of the feature group.\n",
    "\n",
    "- `labels`: Our target variable.\n",
    "\n",
    "- `transformation_functions`: Functions to transform our features.\n",
    "\n",
    "- `query`: A query object containing the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting or creating a feature view named 'electricity_feature_view'\n",
    "version = 1 # Defining the version for the feature view\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='electricity_feature_view',\n",
    "    version=version,\n",
    "    labels=[], # Labels will be defined manually later for our 'y'\n",
    "    transformation_functions=transformation_functions,\n",
    "    query=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `fs.create_training_dataset()` method.\n",
    "\n",
    "**From feature view APIs you can also create training datasts based on even time filters specifing `start_time` and `end_time`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2656a3;\"> ‚õ≥Ô∏è Dataset with train, test and validation splits</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (211.16s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `21`.\n"
     ]
    }
   ],
   "source": [
    "# Splitting the feature view data into train, validation, and test sets\n",
    "# We didn't specify 'labels' in feature view creation, it will therefore return 'None' for Y\n",
    "X_train, X_val, X_test, _, _, _ = feature_view.train_validation_test_split(\n",
    "    train_start=\"2022-01-01\",\n",
    "    train_end=\"2023-06-30\",\n",
    "    validation_start=\"2023-07-01\",\n",
    "    validation_end=\"2023-09-30\",\n",
    "    test_start=\"2023-10-01\",\n",
    "    test_end=\"2023-12-31\",\n",
    "    description='Electricity price prediction dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the training, validation, and test datasets based on the 'timestamp' column\n",
    "X_train.sort_values([\"timestamp\"], inplace=True)\n",
    "X_val.sort_values([\"timestamp\"], inplace=True)\n",
    "X_test.sort_values([\"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the target variable 'dk1_spotpricedkk_kwh' and defineing 'y_train', 'y_val' and 'y_test' \n",
    "y_train = X_train[[\"dk1_spotpricedkk_kwh\"]]\n",
    "y_val = X_val[[\"dk1_spotpricedkk_kwh\"]]\n",
    "y_test = X_test[[\"dk1_spotpricedkk_kwh\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'date', 'time' and 'timestamp' columns from the training, validation, and test datasets\n",
    "X_train.drop([\"date\", \"time\", \"timestamp\"], axis=1, inplace=True)\n",
    "X_val.drop([\"date\", \"time\", \"timestamp\"], axis=1, inplace=True)\n",
    "X_test.drop([\"date\", \"time\", \"timestamp\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'dare', 'time' and 'timestamp' and dependent variable (y) columns from the training, validation, and test datasets\n",
    "X_train.drop([\"dk1_spotpricedkk_kwh\"], axis=1, inplace=True)\n",
    "X_val.drop([\"dk1_spotpricedkk_kwh\"], axis=1, inplace=True)\n",
    "X_test.drop([\"dk1_spotpricedkk_kwh\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>rain</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_gusts_10m</th>\n",
       "      <th>dk1_offshore_wind_forecastintraday_kwh</th>\n",
       "      <th>dk1_onshore_wind_forecastintraday_kwh</th>\n",
       "      <th>dk1_solar_forecastintraday_kwh</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5905751</th>\n",
       "      <td>0.435268</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315152</td>\n",
       "      <td>0.272633</td>\n",
       "      <td>0.945277</td>\n",
       "      <td>0.481878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19398</th>\n",
       "      <td>0.435268</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315152</td>\n",
       "      <td>0.272633</td>\n",
       "      <td>0.934795</td>\n",
       "      <td>0.446702</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919627</th>\n",
       "      <td>0.417411</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082828</td>\n",
       "      <td>0.074922</td>\n",
       "      <td>0.773045</td>\n",
       "      <td>0.264375</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719247</th>\n",
       "      <td>0.426339</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195960</td>\n",
       "      <td>0.187305</td>\n",
       "      <td>0.913059</td>\n",
       "      <td>0.358547</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743896</th>\n",
       "      <td>0.417411</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082828</td>\n",
       "      <td>0.074922</td>\n",
       "      <td>0.493641</td>\n",
       "      <td>0.133456</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         temperature_2m  relative_humidity_2m  precipitation      rain  \\\n",
       "5905751        0.435268              0.986667       0.011364  0.011364   \n",
       "19398          0.435268              0.986667       0.011364  0.011364   \n",
       "5919627        0.417411              0.933333       0.000000  0.000000   \n",
       "4719247        0.426339              0.933333       0.000000  0.000000   \n",
       "4743896        0.417411              0.933333       0.000000  0.000000   \n",
       "\n",
       "         snowfall  weather_code  cloud_cover  wind_speed_10m  wind_gusts_10m  \\\n",
       "5905751       0.0          0.68          1.0        0.315152        0.272633   \n",
       "19398         0.0          0.68          1.0        0.315152        0.272633   \n",
       "5919627       0.0          0.04          1.0        0.082828        0.074922   \n",
       "4719247       0.0          0.04          1.0        0.195960        0.187305   \n",
       "4743896       0.0          0.04          1.0        0.082828        0.074922   \n",
       "\n",
       "         dk1_offshore_wind_forecastintraday_kwh  \\\n",
       "5905751                                0.945277   \n",
       "19398                                  0.934795   \n",
       "5919627                                0.773045   \n",
       "4719247                                0.913059   \n",
       "4743896                                0.493641   \n",
       "\n",
       "         dk1_onshore_wind_forecastintraday_kwh  \\\n",
       "5905751                               0.481878   \n",
       "19398                                 0.446702   \n",
       "5919627                               0.264375   \n",
       "4719247                               0.358547   \n",
       "4743896                               0.133456   \n",
       "\n",
       "         dk1_solar_forecastintraday_kwh  type  \n",
       "5905751                        0.000000     1  \n",
       "19398                          0.000008     1  \n",
       "5919627                        0.000018     1  \n",
       "4719247                        0.000012     1  \n",
       "4743896                        0.005406     1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first 5 rows of the train dataset (X_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">üóÉ Window timeseries dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5012736, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">üß¨ Modeling Testing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15496\\1411499862.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((-1, 1, 13))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15496\\1311144430.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((-1, 13, 1))\n",
    "X_test = X_test.reshape((-1, 13, 1))\n",
    "\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mValueError\u001b[0m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train.astype(float)).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test.astype(float)).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2656a3;\">üß¨ Modeling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the XGBoost regressor\n",
    "# model = xgb.XGBRegressor()\n",
    "# model_val = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model on the training data\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the validation set\n",
    "# y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate RMSE on the validation set\n",
    "# mse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "# print(f\"Mean Squared Error (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#2656a3'>üóÑ Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trained model to: electricity_price_model\n",
      "INFO:tensorflow:Assets written to: electricity_price_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Exporting the trained model to a directory\n",
    "model_dir = \"electricity_price_model\"\n",
    "print('Exporting trained model to: {}'.format(model_dir))\n",
    "\n",
    "# Saving the model using TensorFlow's saved_model.save function\n",
    "tf.saved_model.save(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 elapsed<00:01 remaining<00:001<00:01,  3.38it/s]\n",
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 397272/397272 elapsed<00:02 remaining<00:00  3.38it/s]\n",
      "Uploading: 0.000%|          | 0/112411 elapsed<00:01 remaining<?0:04<00:01,  3.38it/s]\n",
      "Uploading model files (2 dirs, 2 files):  17%|‚ñà‚ñã        | 1/6 [00:07<00:35,  7.08s/it]\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/556180/dataset/upload/%2FProjects%2Fbenjami3%2FModels%2FDK_electricity_price_prediction_model%2F1%5Cvariables). Server response: \nHTTP code: 400, HTTP reason: Invalid URI, body: b''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 16\u001b[0m\n\u001b[0;32m      8\u001b[0m tf_model \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mtensorflow\u001b[38;5;241m.\u001b[39mcreate_model(\n\u001b[0;32m      9\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDK_electricity_price_prediction_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[0;32m     11\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourly electricity price prediction model.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     input_example\u001b[38;5;241m=\u001b[39mn_step_window\u001b[38;5;241m.\u001b[39mexample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Saving the model to the specified directory\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\model.py:101\u001b[0m, in \u001b[0;36mModel.save\u001b[1;34m(self, model_path, await_registration, keep_original_files)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path, await_registration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m480\u001b[39m, keep_original_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Persist this model including model files and metadata to the model registry.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    # Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m        `Model`: The model metadata object.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mawait_registration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_registration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_original_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\engine\\model_engine.py:421\u001b[0m, in \u001b[0;36mModelEngine.save\u001b[1;34m(self, model_instance, model_path, await_registration, keep_original_files)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m be:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_api\u001b[38;5;241m.\u001b[39mrm(model_instance\u001b[38;5;241m.\u001b[39mversion_path)\n\u001b[1;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m be\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel created, explore it at \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mget_url())\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_instance\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\engine\\model_engine.py:385\u001b[0m, in \u001b[0;36mModelEngine.save\u001b[1;34m(self, model_instance, model_path, await_registration, keep_original_files)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# check local relative\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    383\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), model_path)\n\u001b[0;32m    384\u001b[0m ):  \u001b[38;5;66;03m# check local relative\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model_from_local_or_hopsfs_mount\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_original_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate_upload_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_upload_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# check project relative\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_api\u001b[38;5;241m.\u001b[39mpath_exists(\n\u001b[0;32m    393\u001b[0m     model_path\n\u001b[0;32m    394\u001b[0m ):  \u001b[38;5;66;03m# check hdfs relative and absolute\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\engine\\model_engine.py:249\u001b[0m, in \u001b[0;36mModelEngine._save_model_from_local_or_hopsfs_mount\u001b[1;34m(self, model_instance, model_path, keep_original_files, update_upload_progress)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_or_move_hopsfs_model(\n\u001b[0;32m    241\u001b[0m         from_hdfs_model_path\u001b[38;5;241m=\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[0;32m    242\u001b[0m             constants\u001b[38;5;241m.\u001b[39mMODEL_REGISTRY\u001b[38;5;241m.\u001b[39mHOPSFS_MOUNT_PREFIX, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m         update_upload_progress\u001b[38;5;241m=\u001b[39mupdate_upload_progress,\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_local_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_local_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_model_version_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate_upload_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_upload_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\engine\\model_engine.py:225\u001b[0m, in \u001b[0;36mModelEngine._upload_local_model\u001b[1;34m(self, from_local_model_path, to_model_version_path, update_upload_progress)\u001b[0m\n\u001b[0;32m    223\u001b[0m     update_upload_progress(n_dirs, n_files)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f_name \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_base_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     n_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    227\u001b[0m     update_upload_progress(n_dirs, n_files)\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\engine\\local_engine.py:38\u001b[0m, in \u001b[0;36mLocalEngine.upload\u001b[1;34m(self, local_path, remote_path)\u001b[0m\n\u001b[0;32m     36\u001b[0m local_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_abs_path(local_path)\n\u001b[0;32m     37\u001b[0m remote_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepend_project_path(remote_path)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\core\\dataset_api.py:152\u001b[0m, in \u001b[0;36mDatasetApi.upload\u001b[1;34m(self, local_path, upload_path, overwrite, chunk_size, simultaneous_uploads, max_chunk_retries, chunk_retry_interval)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m                 pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 152\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\core\\dataset_api.py:148\u001b[0m, in \u001b[0;36mDatasetApi.upload\u001b[1;34m(self, local_path, upload_path, overwrite, chunk_size, simultaneous_uploads, max_chunk_retries, chunk_retry_interval)\u001b[0m\n\u001b[0;32m    146\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m wait(futures)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\core\\dataset_api.py:148\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    146\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m wait(futures)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     _ \u001b[38;5;241m=\u001b[39m [\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\core\\dataset_api.py:189\u001b[0m, in \u001b[0;36mDatasetApi._upload_chunk\u001b[1;34m(self, base_params, upload_path, file_name, chunk, pbar, max_chunk_retries, chunk_retry_interval)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    185\u001b[0m     re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m DatasetApi\u001b[38;5;241m.\u001b[39mFLOW_PERMANENT_ERRORS\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mretries \u001b[38;5;241m>\u001b[39m max_chunk_retries\n\u001b[0;32m    187\u001b[0m ):\n\u001b[0;32m    188\u001b[0m     chunk\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m re\n\u001b[0;32m    190\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(chunk_retry_interval)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\core\\dataset_api.py:178\u001b[0m, in \u001b[0;36mDatasetApi._upload_chunk\u001b[1;34m(self, base_params, upload_path, file_name, chunk, pbar, max_chunk_retries, chunk_retry_interval)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\core\\dataset_api.py:214\u001b[0m, in \u001b[0;36mDatasetApi._upload_request\u001b[1;34m(self, params, path, file_name, chunk)\u001b[0m\n\u001b[0;32m    211\u001b[0m path_params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m, _client\u001b[38;5;241m.\u001b[39m_project_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload\u001b[39m\u001b[38;5;124m\"\u001b[39m, path]\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Flow configuration params are sent as form data\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[1;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Benj3\\anaconda3\\envs\\energy\\Lib\\site-packages\\hsml\\client\\base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[1;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[0;32m    105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39msend(prepped, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/556180/dataset/upload/%2FProjects%2Fbenjami3%2FModels%2FDK_electricity_price_prediction_model%2F1%5Cvariables). Server response: \nHTTP code: 400, HTTP reason: Invalid URI, body: b''"
     ]
    }
   ],
   "source": [
    "# Retrieving the Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Extracting loss value from the training history\n",
    "metrics = {'loss': history_dict['val_loss'][0]} \n",
    "\n",
    "# Creating a TensorFlow model in the Model Registry\n",
    "tf_model = mr.tensorflow.create_model(\n",
    "    name=\"DK_electricity_price_prediction_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Hourly electricity price prediction model.\",\n",
    "    input_example=n_step_window.example[0].numpy(),\n",
    ")\n",
    "\n",
    "# Saving the model to the specified directory\n",
    "tf_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#2656a3;\">‚è≠Ô∏è **Next:** Part 04: Batch Inference </span>\n",
    "\n",
    "In the next notebook you will use your registered model to predict batch data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bds-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
